{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Collection using webscraping and API\n",
    "Name: Samujjal Seal Sarma\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping Youtube - Matching Videos Details\n",
    "  Steps:\n",
    "     (Step 1) : Set Webdriver path for chromedriver accordingly\n",
    "     (Step 2) : Set Control_key value accordingly. If running on MAC, set Control_key = Keys.COMMAND. If running on\n",
    "               windows, set Control_key=Keys.COMMAND\n",
    "     (Step 3) : Using chromedriver, open youtube url\n",
    "     (Step 4) : Enter search_text as 'Kishore Kumar' and submit page\n",
    "     (Step 5) : In search results, run a loop 10 times to fetch each video search result details \n",
    "     (Step 6) : Open each search result url above, and open video in new window\n",
    "     (Step 7) : Scroll down the video link to load comments. Check number of comments and load max 50 comments\n",
    "     (Step 8) : Read below list of values for each video into rowlist\n",
    "             [href, title, subscriptions,viewsCount, lastUpdated, comments, author, postedTime, replyCount, overallvote]           \n",
    "     (Step 9) : Create dataframe out of rowlist and columnlist variables above\n",
    "     (Step 10): Write data from dataframe created above to output_scraping.csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import selenium.webdriver.support.ui as ui\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "import csv\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted search for search_text\n",
      "Video:  1\n",
      "Opening Video:  1\n",
      "Switching driver to Video:  1\n",
      "Switching page for Video:  1\n",
      "Getting number of comments for Video:  1\n",
      "no_of_comments  1876\n",
      "no_of_comments  50\n",
      "Loading comments\n",
      "Finished comments extraction\n",
      "Video:  2\n",
      "Opening Video:  2\n",
      "Switching driver to Video:  2\n",
      "Switching page for Video:  2\n",
      "Getting number of comments for Video:  2\n",
      "no_of_comments  2409\n",
      "no_of_comments  50\n",
      "Loading comments\n",
      "Finished comments extraction\n",
      "Video:  3\n",
      "Opening Video:  3\n",
      "Switching driver to Video:  3\n",
      "Switching page for Video:  3\n",
      "Getting number of comments for Video:  3\n",
      "no_of_comments  648\n",
      "no_of_comments  50\n",
      "Loading comments\n",
      "Finished comments extraction\n",
      "Video:  4\n",
      "Opening Video:  4\n",
      "Switching driver to Video:  4\n",
      "Switching page for Video:  4\n",
      "Getting number of comments for Video:  4\n",
      "no_of_comments  3245\n",
      "no_of_comments  50\n",
      "Loading comments\n",
      "Finished comments extraction\n",
      "Video:  5\n",
      "Opening Video:  5\n",
      "Switching driver to Video:  5\n",
      "Switching page for Video:  5\n",
      "Getting number of comments for Video:  5\n",
      "no_of_comments  483\n",
      "no_of_comments  50\n",
      "Loading comments\n",
      "Finished comments extraction\n",
      "Video:  6\n",
      "Opening Video:  6\n",
      "Switching driver to Video:  6\n",
      "Switching page for Video:  6\n",
      "Getting number of comments for Video:  6\n",
      "no_of_comments  583\n",
      "no_of_comments  50\n",
      "Loading comments\n",
      "Finished comments extraction\n",
      "Video:  7\n",
      "Opening Video:  7\n",
      "Switching driver to Video:  7\n",
      "Switching page for Video:  7\n",
      "Getting number of comments for Video:  7\n",
      "no_of_comments  356\n",
      "no_of_comments  50\n",
      "Loading comments\n",
      "Finished comments extraction\n",
      "Video:  8\n",
      "Opening Video:  8\n",
      "Switching driver to Video:  8\n",
      "Switching page for Video:  8\n",
      "Getting number of comments for Video:  8\n",
      "no_of_comments  9362\n",
      "no_of_comments  50\n",
      "Loading comments\n",
      "Finished comments extraction\n",
      "Video:  9\n",
      "Opening Video:  9\n",
      "Switching driver to Video:  9\n",
      "Switching page for Video:  9\n",
      "Getting number of comments for Video:  9\n",
      "no_of_comments  5679\n",
      "no_of_comments  50\n",
      "Loading comments\n",
      "Finished comments extraction\n",
      "Video:  10\n",
      "Opening Video:  10\n",
      "Switching driver to Video:  10\n",
      "Switching page for Video:  10\n",
      "Getting number of comments for Video:  10\n",
      "no_of_comments  1825\n",
      "no_of_comments  50\n",
      "Loading comments\n",
      "Finished comments extraction\n"
     ]
    }
   ],
   "source": [
    "url = \"https://www.youtube.com\"\n",
    "\n",
    "# Set chrome driver path accordingly\n",
    "driver = webdriver.Chrome('D://ISB CBA/Term 1/DC/chromedriver.exe')\n",
    "# For running this code on MAC Machine, Set Control_key to below value. On MAC, command+enter is used to open new window url\n",
    "#Control_key = Keys.COMMAND\n",
    "#For running this code on Windows Machine, Set Control_key to below value. On Windows, control+enter is used to open new window url\n",
    "Control_key = Keys.CONTROL\n",
    "\n",
    "driver.get(url)\n",
    "search_text = \"Kishore kumar\"\n",
    "\n",
    "#driver.maximize_window()\n",
    "textbox = driver.find_element_by_id(\"search\").send_keys(search_text)\n",
    "button = driver.find_element_by_id(\"search-icon-legacy\")\n",
    "\n",
    "button.submit()\n",
    "print(\"Submitted search for search_text\")\n",
    "#page1_results = driver.find_elements(By.XPATH, '//*[@id=\"contents\"]')\n",
    "time.sleep(3)\n",
    "\n",
    "videoCounter=1 \n",
    "rowlist = []\n",
    "\n",
    "while videoCounter <= 10:\n",
    "    print(\"Video: \",videoCounter)\n",
    "    search_list = []\n",
    "    try:\n",
    "        search_list = driver.find_elements_by_xpath('//*[@id=\"contents\"]/ytd-video-renderer['+str(videoCounter)+']')[0]\n",
    "        rowInitial = []\n",
    "        try:#url\n",
    "            rowInitial.append(search_list.find_element_by_xpath('.//*[@id=\"video-title\"]').get_property(\"href\"))\n",
    "        except:\n",
    "            rowInitial.append(\"\")\n",
    "        try:#subscription\n",
    "            rowInitial.append(search_list.find_element_by_xpath('.//*[@id=\"byline\"]/a').text)\n",
    "        except:\n",
    "            row.append(\"\")\n",
    "        try:#title\n",
    "            rowInitial.append(search_list.find_element_by_xpath('.//*[@id=\"video-title\"]').text)\n",
    "        except:\n",
    "            rowInitial.append(\"\")\n",
    "        try:#views count\n",
    "            rowInitial.append(search_list.find_element_by_xpath('.//*[@id=\"metadata-line\"]/span[1]').text)\n",
    "        except:\n",
    "            rowInitial.append(\"\")\n",
    "        try:#last updated\n",
    "            rowInitial.append(search_list.find_element_by_xpath('.//*[@id=\"metadata-line\"]/span[2]').text)\n",
    "        except:\n",
    "            rowInitial.append(\"\")\n",
    "        print(\"Opening Video: \",videoCounter)\n",
    "        search_list.find_element_by_xpath('.//*[@id=\"video-title\"]').send_keys(Control_key + Keys.RETURN)\n",
    "        time.sleep(3)\n",
    "        print(\"Switching driver to Video: \",videoCounter)\n",
    "        driver.switch_to.window(driver.window_handles[1]);\n",
    "        time.sleep(2)\n",
    "        #scroll page till this first scroll element\n",
    "        print(\"Switching page for Video: \",videoCounter)\n",
    "        prev_scroll = driver.find_elements_by_xpath('//*[@id=\"more\"]/yt-formatted-string')[0].location['y']\n",
    "        driver.execute_script(\"window.scrollTo(0,\"+ str(prev_scroll)+ \")\")\n",
    "        time.sleep(4)\n",
    "        \n",
    "        #get number of available comments\n",
    "        print(\"Getting number of comments for Video: \",videoCounter)\n",
    "        counts_element = driver.find_elements_by_xpath('//*[@id=\"count\"]/yt-formatted-string')[0]\n",
    "        no_of_comments = int(counts_element.text.split( )[0].replace(',',''))\n",
    "        prev_scroll = counts_element.location['x']\n",
    "        print(\"no_of_comments \", no_of_comments)\n",
    "        if no_of_comments > 50:\n",
    "            no_of_comments = 50\n",
    "        \n",
    "        #append comments of each link\n",
    "        commentsCounter = 1\n",
    "        print(\"no_of_comments \", no_of_comments)\n",
    "\n",
    "        print('Loading comments')\n",
    "        while commentsCounter <= no_of_comments: \n",
    "            row = []\n",
    "            row = rowInitial.copy()\n",
    "            try:\n",
    "                next_comment = driver.find_elements_by_xpath('//*[@id=\"contents\"]/ytd-comment-thread-renderer['+str(commentsCounter)+']')[0]\n",
    "                try:#comment\n",
    "                    row.append(next_comment.find_elements_by_xpath('.//*[@id=\"content-text\"]')[0].text)\n",
    "                except:\n",
    "                    row.append(\"\")\n",
    "                try:#author\n",
    "                     row.append(next_comment.find_elements_by_xpath('.//*[@id=\"author-text\"]')[0].text)\n",
    "                except:\n",
    "                    row.append(\"\")            \n",
    "                try:#published time\n",
    "                    row.append(next_comment.find_elements_by_xpath('.//*[@id=\"published-time-text\"]/a')[0].text)\n",
    "                except:\n",
    "                    row.append(\"\")\n",
    "                try:#reply count\n",
    "                    row.append(next_comment.find_elements_by_xpath('.//*[@id=\"more\"]/div/paper-button')[0].text)\n",
    "                except:\n",
    "                    row.append(\"\")\n",
    "                try:#vote count\n",
    "                    row.append(next_comment.find_elements_by_xpath('.//*[@id=\"vote-count-middle\"]')[0].text)\n",
    "                except:\n",
    "                    row.append(\"\")\n",
    "                rowlist.append(row)\n",
    "                commentsCounter += 1\n",
    "            except:\n",
    "                #print(\"Next comments element not found, so loading again\")\n",
    "                i = 0\n",
    "                while i < 2:\n",
    "                    try:\n",
    "                        parent_element = driver.find_elements_by_xpath('//*[@id=\"continuations\"]')[1]\n",
    "                        prev_scroll += parent_element.location['x']\n",
    "                        driver.execute_script(\"window.scrollTo(\"+ str(prev_scroll) + \",\" + str(prev_scroll+parent_element.location['x']) + \")\")                    \n",
    "                        i+=1\n",
    "                        time.sleep(2)\n",
    "                    except:\n",
    "                        if commentsCounter<=50:\n",
    "                            try:\n",
    "                                print(\"in except try \",driver.find_elements_by_xpath('//*[@id=\"contents\"]/ytd-comment-thread-renderer['+str(count1)+']')[0])\n",
    "                                commentsCounter += 1\n",
    "                            except:\n",
    "                                commentsCounter = commentsCounter+1\n",
    "               \n",
    "    except Exception as e:\n",
    "        print(\"Exception \"+str(e))\n",
    "    print('Finished comments extraction')   \n",
    "    driver.close();\n",
    "    driver.switch_to.window(driver.window_handles[0]);\n",
    "    videoCounter += 1\n",
    "#insert comments data into csv file\n",
    "columns_list = [\"href\",\"title\",\"subscriptions\",\"viewsCount\",\"lastUpdated\",\"conmments\",\"author\",\"postedTime\",\"replyCount\",\"overallvote\"]\n",
    "data = pd.DataFrame(rowlist, columns=columns_list )   \n",
    "data.to_csv('output_scraping.csv', index=False, columns=columns_list)\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping Articles using - News API\n",
    "  Steps:\n",
    "     (Step1)  : Generate API Key for NewsApiClient\n",
    "     (Step2)  : Call newsapi.get_everything with query q as 'business analytics' and sort_by='publishedAt' to sort \n",
    "               by recent ones\n",
    "     (Step 3) : In response returned, check totalResults returned to find number of matching articles\n",
    "     (Step 4) : newsAPI returns 20 articles per page. So, in results returned, loop through        totalResults/defaultPageSize times. Note, defaultPageSize = 20\n",
    "     (Step 5) : Each time in for loop, read fields - source id, source name, author, title, description and content\n",
    "     (Step 6) : Write above article fields to articleRow\n",
    "     (Step 7) : Append each of the articles to articleList\n",
    "     (Step 8) : newsapi limits number of matching article responses to 1000 (till Page=50)\n",
    "     (Step 9) : Convert articleList to dataframe object\n",
    "     (Step 10): Write articleDF object to output_api file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "URL: https://newsapi.org/v2/everything?q=business%20analytics&apiKey=3a8a868b31cd4975973a0cab97e98a91&language=en\n",
    "\n",
    "APIKEY: 3a8a868b31cd4975973a0cab97e98a91\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from newsapi import NewsApiClient\n",
    "try:\n",
    "    maxfreeArticlesPageCountLimit = 50\n",
    "    defaultPageSize = 20\n",
    "    newsapi = NewsApiClient(api_key='3a8a868b31cd4975973a0cab97e98a91')\n",
    "    newsapiResponse = newsapi.get_everything(q='business analytics',\n",
    "                                      language='en',\n",
    "                                      sort_by='publishedAt')\n",
    "    totalResults = newsapiResponse['totalResults']\n",
    "    if(totalResults > 0):\n",
    "        numOfArticlePages = totalResults/defaultPageSize\n",
    "        pageNumber = 1\n",
    "        articlesList = []\n",
    "        while (pageNumber <= numOfArticlePages) and (pageNumber <= maxfreeArticlesPageCountLimit):\n",
    "            newsapiResponse = newsapi.get_everything(q='business analytics',\n",
    "                                      language='en',\n",
    "                                      sort_by='publishedAt',\n",
    "                                      page=pageNumber)\n",
    "            newapiResponseStatus = newsapiResponse['status']\n",
    "            if(newapiResponseStatus == 'ok'):\n",
    "                numOfArticlesInCurrentPage = len(newsapiResponse['articles'])\n",
    "                pageLevelArticleCounter = 0\n",
    "                while pageLevelArticleCounter < numOfArticlesInCurrentPage:\n",
    "                    articleRow = []\n",
    "                    articleRow.append(newsapiResponse['articles'][pageLevelArticleCounter]['source']['id'])\n",
    "                    articleRow.append(newsapiResponse['articles'][pageLevelArticleCounter]['source']['name'])\n",
    "                    articleRow.append(newsapiResponse['articles'][pageLevelArticleCounter]['author'])\n",
    "                    articleRow.append(newsapiResponse['articles'][pageLevelArticleCounter]['title'])\n",
    "                    articleRow.append(newsapiResponse['articles'][pageLevelArticleCounter]['description'])\n",
    "                    articleRow.append(newsapiResponse['articles'][pageLevelArticleCounter]['content'])\n",
    "                    articlesList.append(articleRow)\n",
    "                    pageLevelArticleCounter += 1\n",
    "                pageNumber += 1    \n",
    "        if(len(articlesList)>0):\n",
    "            columns_list = [\"Source ID\",\"Source Name\",\"Auhtor\",\"Title\",\"Description\",\"Contents\"]\n",
    "            articleDF = pd.DataFrame(articlesList, columns=columns_list)   \n",
    "            articleDF.to_csv(\"output_api.csv\", index=False, columns=columns_list)\n",
    "            print(str(totalResults)+\" articles match found.\")\n",
    "            print(\"Successfully Inserted: \"+str(len(articlesList))+\" articles to  output_api.csv file based on maxfree limit permitted by newsAPI\")\n",
    "        else:\n",
    "            print('No articles found')\n",
    "    else:\n",
    "        print('totalResults returned: '+str(totalResults)+' is not > 0')\n",
    "except Exception as e:\n",
    "    print('Exception raised: '+str(e))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
